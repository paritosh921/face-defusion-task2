{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpPADnGVt9OX",
        "outputId": "f2e94319-da62-4a50-f29f-9facc96ca304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (for Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IN_COLAB = True\n",
        "    print(\"Google Drive mounted successfully\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Not running in Google Colab or drive already mounted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zsq7I7mYuDsS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X07pvQi2qGeC",
        "outputId": "36cfa1b3-6255-4074-a2d1-8c7d4289259a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "import math\n",
        "\n",
        "# Ignore specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "id": "hYeTRYmbV3cr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print system info\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGxILJ9tms5b",
        "outputId": "cb644816-1e81-42df-985e-87b6e67c9894"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset implementation\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, img_size=224):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the face images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "            img_size (int): Size of images to resize to\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        print(f\"Found {len(self.image_files)} images in {root_dir}\")\n",
        "\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "def get_dataloaders(data_dir, batch_size=32, img_size=224, train_ratio=0.9):\n",
        "    \"\"\"\n",
        "    Create train and validation data loaders\n",
        "    \"\"\"\n",
        "    dataset = FaceDataset(root_dir=data_dir, img_size=img_size)\n",
        "\n",
        "    # Split into train and validation sets\n",
        "    train_size = int(train_ratio * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# Encoder implementation (similar to previous code)\n",
        "class DINOv2FaceEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, finetune=True):\n",
        "        super().__init__()\n",
        "        # Initialize with pretrained DINOv2 weights\n",
        "        self.model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Remove the classification head\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.feature_dim = self.model.heads.head.in_features\n",
        "\n",
        "        # Replace head with identity\n",
        "        self.model.heads = nn.Identity()\n",
        "\n",
        "        # Projection layer to desired embedding dimension\n",
        "        if embedding_dim != self.feature_dim:\n",
        "            self.projection = nn.Linear(self.feature_dim, embedding_dim)\n",
        "        else:\n",
        "            self.projection = nn.Identity()\n",
        "\n",
        "        # Freeze or unfreeze the model\n",
        "        if not finetune:\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.model(x)\n",
        "        embeddings = self.projection(features)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "9UYIQ1zrVy2G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diffusion model implementation (similar to previous code)\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "UEf7z0fFVueZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalUNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, base_channels=64,\n",
        "                 time_dim=256, embedding_dim=768, device=\"cuda\"):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.time_dim = time_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(time_dim),\n",
        "            nn.Linear(time_dim, time_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(time_dim, time_dim)\n",
        "        )\n",
        "\n",
        "        # Condition embedding\n",
        "        self.cond_encoder = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, time_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(time_dim, time_dim),\n",
        "        )\n",
        "\n",
        "        # Downsampling and upsampling paths\n",
        "        self.downs = nn.ModuleList([\n",
        "            self._down_block(in_channels, base_channels),\n",
        "            self._down_block(base_channels, base_channels * 2),\n",
        "            self._down_block(base_channels * 2, base_channels * 4),\n",
        "            self._down_block(base_channels * 4, base_channels * 8)\n",
        "        ])\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(base_channels * 8, base_channels * 8, 3, padding=1),\n",
        "            nn.BatchNorm2d(base_channels * 8),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(base_channels * 8, base_channels * 8, 3, padding=1),\n",
        "            nn.BatchNorm2d(base_channels * 8),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Upsampling path with skip connections\n",
        "        self.ups = nn.ModuleList([\n",
        "            self._up_block(base_channels * 16, base_channels * 4),\n",
        "            self._up_block(base_channels * 8, base_channels * 2),\n",
        "            self._up_block(base_channels * 4, base_channels),\n",
        "            self._up_block(base_channels * 2, base_channels)\n",
        "        ])\n",
        "\n",
        "        # Output convolution\n",
        "        self.final_conv = nn.Conv2d(base_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def _down_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "    def _up_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, timestep, embedding):\n",
        "        # Encode time\n",
        "        t = self.time_mlp(timestep)\n",
        "\n",
        "        # Encode conditioning\n",
        "        c = self.cond_encoder(embedding)\n",
        "\n",
        "        # Combine time and conditioning\n",
        "        t = t + c\n",
        "\n",
        "        # Store skip connections\n",
        "        skip_connections = []\n",
        "\n",
        "        # Downsampling\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Upsampling with skip connections\n",
        "        for up, skip in zip(self.ups, reversed(skip_connections)):\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "            x = up(x)\n",
        "\n",
        "        # Final convolution\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "UV6xvt8jVpej"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionModel:\n",
        "    def __init__(self, model, beta_start=1e-4, beta_end=0.02, num_diffusion_steps=1000, device=\"cuda\"):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.num_diffusion_steps = num_diffusion_steps\n",
        "\n",
        "        # Define beta schedule\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_diffusion_steps).to(device)\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
        "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.)\n",
        "\n",
        "        # Calculations for diffusion\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
        "\n",
        "    def get_noisy_image(self, x_start, t):\n",
        "        x_start = x_start.to(self.device)\n",
        "        noise = torch.randn_like(x_start).to(self.device)\n",
        "\n",
        "        return (\n",
        "            self.sqrt_alphas_cumprod[t, None, None, None] * x_start +\n",
        "            self.sqrt_one_minus_alphas_cumprod[t, None, None, None] * noise,\n",
        "            noise\n",
        "        )\n",
        "\n",
        "    def train_step(self, clean_images, embeddings, optimizer, criterion):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Sample random timesteps\n",
        "        batch_size = clean_images.shape[0]\n",
        "        t = torch.randint(0, self.num_diffusion_steps, (batch_size,), device=self.device).long()\n",
        "\n",
        "        # Get noisy image and true noise to predict\n",
        "        noisy_images, true_noise = self.get_noisy_image(clean_images, t)\n",
        "\n",
        "        # Predict noise\n",
        "        pred_noise = self.model(noisy_images, t, embeddings)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(pred_noise, true_noise)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, embedding, image_size=224, batch_size=1, channels=3):\n",
        "        # Start with random noise\n",
        "        img = torch.randn(batch_size, channels, image_size, image_size).to(self.device)\n",
        "        embedding = embedding.to(self.device)\n",
        "\n",
        "        # Iterative denoising\n",
        "        for i in tqdm(reversed(range(0, self.num_diffusion_steps)), desc='Sampling'):\n",
        "            timesteps = torch.full((batch_size,), i, device=self.device, dtype=torch.long)\n",
        "\n",
        "            # Get model prediction (predicted noise)\n",
        "            predicted_noise = self.model(img, timesteps, embedding)\n",
        "\n",
        "            # Compute denoise step\n",
        "            alpha = self.alphas[i]\n",
        "            alpha_cumprod = self.alphas_cumprod[i]\n",
        "            beta = self.betas[i]\n",
        "\n",
        "            if i > 0:\n",
        "                noise = torch.randn_like(img)\n",
        "            else:\n",
        "                noise = torch.zeros_like(img)\n",
        "\n",
        "            # Compute step according to the formula\n",
        "            img = (1 / torch.sqrt(alpha)) * (\n",
        "                img - ((1 - alpha) / torch.sqrt(1 - alpha_cumprod)) * predicted_noise\n",
        "            ) + torch.sqrt(beta) * noise\n",
        "\n",
        "        # Scale image to [0, 1]\n",
        "        img = (img.clamp(-1, 1) + 1) / 2\n",
        "\n",
        "        return img"
      ],
      "metadata": {
        "id": "5PIszoI7Vill"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_face_generation(\n",
        "    data_dir='/content/drive/MyDrive/img_align_celeba',\n",
        "    output_dir='outputs',\n",
        "    batch_size=32,\n",
        "    img_size=224,\n",
        "    embedding_dim=768,\n",
        "    encoder_epochs=10,\n",
        "    diffusion_epochs=10,\n",
        "    device='cuda'\n",
        "):\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Get dataloaders\n",
        "    train_loader, val_loader = get_dataloaders(\n",
        "        data_dir,\n",
        "        batch_size=batch_size,\n",
        "        img_size=img_size\n",
        "    )\n",
        "\n",
        "    # Initialize encoder\n",
        "    encoder = DINOv2FaceEncoder(embedding_dim=embedding_dim, finetune=True).to(device)\n",
        "\n",
        "    # Initialize diffusion model\n",
        "    unet = ConditionalUNet(\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        base_channels=64,\n",
        "        time_dim=256,\n",
        "        embedding_dim=embedding_dim,\n",
        "        device=device\n",
        "    ).to(device)\n",
        "\n",
        "    diffusion = DiffusionModel(unet, num_diffusion_steps=1000, device=device)\n",
        "\n",
        "    # Optimizers and loss\n",
        "    encoder_optimizer = optim.AdamW(encoder.parameters(), lr=1e-4)\n",
        "    diffusion_optimizer = optim.AdamW(diffusion.model.parameters(), lr=2e-4)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(diffusion_epochs):\n",
        "        # Generate embeddings and train diffusion model\n",
        "        encoder.train()\n",
        "        diffusion.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{diffusion_epochs}\"):\n",
        "            images = batch.to(device)\n",
        "\n",
        "            # Generate embeddings\n",
        "            with torch.no_grad():\n",
        "                embeddings = encoder(images)\n",
        "\n",
        "            # Train diffusion model\n",
        "            loss = diffusion.train_step(images, embeddings, diffusion_optimizer, criterion)\n",
        "            total_loss += loss\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "        # Periodically generate and save samples\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            diffusion.model.eval()\n",
        "            with torch.no_grad():\n",
        "                # Sample some embeddings from validation set\n",
        "                val_batch = next(iter(val_loader)).to(device)\n",
        "                val_embeddings = encoder(val_batch)\n",
        "\n",
        "                # Generate samples\n",
        "                samples = diffusion.sample(val_embeddings[:16], image_size=img_size)\n",
        "\n",
        "                # Save samples\n",
        "                save_path = os.path.join(output_dir, f'samples_epoch_{epoch+1}.png')\n",
        "                plt.figure(figsize=(10, 10))\n",
        "                grid = make_grid(samples, nrow=4, normalize=True)\n",
        "                plt.imshow(grid.permute(1, 2, 0).cpu())\n",
        "                plt.axis('off')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(save_path)\n",
        "                plt.close()\n",
        "\n",
        "    # Save final models\n",
        "    torch.save({\n",
        "        'encoder': encoder.state_dict(),\n",
        "        'diffusion': diffusion.model.state_dict()\n",
        "    }, os.path.join(output_dir, 'final_model.pth'))\n",
        "\n",
        "    return encoder, diffusion\n",
        "\n",
        "def make_grid(tensor, nrow=8, padding=2, normalize=False, scale_each=False, pad_value=0):\n",
        "    \"\"\"\n",
        "    Make a grid of images from a tensor\n",
        "    \"\"\"\n",
        "    from torchvision.utils import make_grid as tv_make_grid\n",
        "    return tv_make_grid(tensor, nrow=nrow, padding=padding,\n",
        "                        normalize=normalize,\n",
        "                        scale_each=scale_each,\n",
        "                        pad_value=pad_value)"
      ],
      "metadata": {
        "id": "abwZGulhVaLg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Mt_ahw0xvc98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b1c2beb-b46d-4b19-ed89-34f2b84dd5b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5434 images in /content/drive/MyDrive/img_align_celeba\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:02<00:00, 150MB/s]\n",
            "Epoch 1/10: 100%|██████████| 153/153 [03:54<00:00,  1.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 0.3194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 153/153 [01:47<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Average Loss: 0.1108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 153/153 [01:47<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Average Loss: 0.0850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 153/153 [01:46<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Average Loss: 0.0744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 153/153 [01:46<00:00,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Average Loss: 0.0658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Sampling: 1000it [00:04, 209.75it/s]\n",
            "Epoch 6/10: 100%|██████████| 153/153 [01:47<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Average Loss: 0.0605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 153/153 [01:47<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Average Loss: 0.0566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 153/153 [01:46<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Average Loss: 0.0513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 153/153 [01:46<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Average Loss: 0.0510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 153/153 [01:46<00:00,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Average Loss: 0.0455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Sampling: 1000it [00:04, 220.22it/s]\n"
          ]
        }
      ],
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specific path to face dataset\n",
        "    data_dir = '/content/drive/MyDrive/img_align_celeba'\n",
        "\n",
        "    # Train the face generation model\n",
        "    encoder, diffusion_model = train_face_generation(\n",
        "        data_dir=data_dir,\n",
        "        output_dir='face_generation_outputs',\n",
        "        batch_size=32,\n",
        "        img_size=224,\n",
        "        embedding_dim=768,\n",
        "        encoder_epochs=10,\n",
        "        diffusion_epochs=10\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "import math\n",
        "\n",
        "# Ignore specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Dataset implementation\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, img_size=224):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the face images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "            img_size (int): Size of images to resize to\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        print(f\"Found {len(self.image_files)} images in {root_dir}\")\n",
        "\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "def get_dataloaders(data_dir, batch_size=32, img_size=224, train_ratio=0.9):\n",
        "    \"\"\"\n",
        "    Create train and validation data loaders\n",
        "    \"\"\"\n",
        "    dataset = FaceDataset(root_dir=data_dir, img_size=img_size)\n",
        "    train_size = int(train_ratio * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# Encoder implementation\n",
        "class DINOv2FaceEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, finetune=True):\n",
        "        super().__init__()\n",
        "        # Initialize with pretrained DINOv2 weights\n",
        "        self.model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "        # Remove the classification head\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.feature_dim = self.model.heads.head.in_features\n",
        "        # Replace head with identity\n",
        "        self.model.heads = nn.Identity()\n",
        "        # Projection layer to desired embedding dimension\n",
        "        if embedding_dim != self.feature_dim:\n",
        "            self.projection = nn.Linear(self.feature_dim, embedding_dim)\n",
        "        else:\n",
        "            self.projection = nn.Identity()\n",
        "        # Freeze or unfreeze the model\n",
        "        if not finetune:\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.model(x)\n",
        "        embeddings = self.projection(features)\n",
        "        return embeddings\n",
        "\n",
        "# Diffusion model components\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "class ConditionalUNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, base_channels=64,\n",
        "                 time_dim=256, embedding_dim=768, device=\"cuda\"):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.time_dim = time_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        # Time embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(time_dim),\n",
        "            nn.Linear(time_dim, time_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(time_dim, time_dim)\n",
        "        )\n",
        "        # Condition embedding\n",
        "        self.cond_encoder = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, time_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(time_dim, time_dim),\n",
        "        )\n",
        "        # Downsampling and upsampling paths\n",
        "        self.downs = nn.ModuleList([\n",
        "            self._down_block(in_channels, base_channels),\n",
        "            self._down_block(base_channels, base_channels * 2),\n",
        "            self._down_block(base_channels * 2, base_channels * 4),\n",
        "            self._down_block(base_channels * 4, base_channels * 8)\n",
        "        ])\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(base_channels * 8, base_channels * 8, 3, padding=1),\n",
        "            nn.BatchNorm2d(base_channels * 8),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(base_channels * 8, base_channels * 8, 3, padding=1),\n",
        "            nn.BatchNorm2d(base_channels * 8),\n",
        "            nn.GELU()\n",
        "        )\n",
        "        # Upsampling with skip connections\n",
        "        self.ups = nn.ModuleList([\n",
        "            self._up_block(base_channels * 16, base_channels * 4),\n",
        "            self._up_block(base_channels * 8, base_channels * 2),\n",
        "            self._up_block(base_channels * 4, base_channels),\n",
        "            self._up_block(base_channels * 2, base_channels)\n",
        "        ])\n",
        "        # Output convolution\n",
        "        self.final_conv = nn.Conv2d(base_channels, out_channels, kernel_size=1)\n",
        "    def _down_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "    def _up_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "    def forward(self, x, timestep, embedding):\n",
        "        # Encode time\n",
        "        t = self.time_mlp(timestep)\n",
        "        # Encode conditioning\n",
        "        c = self.cond_encoder(embedding)\n",
        "        # Combine time and conditioning\n",
        "        t = t + c\n",
        "        # Store skip connections\n",
        "        skip_connections = []\n",
        "        # Downsampling\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "        # Upsampling with skip connections\n",
        "        for up, skip in zip(self.ups, reversed(skip_connections)):\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "            x = up(x)\n",
        "        # Final convolution\n",
        "        return self.final_conv(x)\n",
        "\n",
        "class DiffusionModel:\n",
        "    def __init__(self, model, beta_start=1e-4, beta_end=0.02, num_diffusion_steps=1000, device=\"cuda\"):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.num_diffusion_steps = num_diffusion_steps\n",
        "        # Define beta schedule\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_diffusion_steps).to(device)\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
        "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.)\n",
        "        # Calculations for diffusion\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
        "    def get_noisy_image(self, x_start, t):\n",
        "        x_start = x_start.to(self.device)\n",
        "        noise = torch.randn_like(x_start).to(self.device)\n",
        "        return (\n",
        "            self.sqrt_alphas_cumprod[t, None, None, None] * x_start +\n",
        "            self.sqrt_one_minus_alphas_cumprod[t, None, None, None] * noise,\n",
        "            noise\n",
        "        )\n",
        "    def train_step(self, clean_images, embeddings, optimizer, criterion):\n",
        "        optimizer.zero_grad()\n",
        "        # Sample random timesteps\n",
        "        batch_size = clean_images.shape[0]\n",
        "        t = torch.randint(0, self.num_diffusion_steps, (batch_size,), device=self.device).long()\n",
        "        # Get noisy image and true noise to predict\n",
        "        noisy_images, true_noise = self.get_noisy_image(clean_images, t)\n",
        "        # Predict noise\n",
        "        pred_noise = self.model(noisy_images, t, embeddings)\n",
        "        # Calculate loss\n",
        "        loss = criterion(pred_noise, true_noise)\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        return loss.item()\n",
        "    @torch.no_grad()\n",
        "    def sample(self, embedding, image_size=224, batch_size=1, channels=3):\n",
        "        # Start with random noise\n",
        "        img = torch.randn(batch_size, channels, image_size, image_size).to(self.device)\n",
        "        embedding = embedding.to(self.device)\n",
        "        # Iterative denoising\n",
        "        for i in tqdm(reversed(range(0, self.num_diffusion_steps)), desc='Sampling'):\n",
        "            timesteps = torch.full((batch_size,), i, device=self.device, dtype=torch.long)\n",
        "            # Get model prediction (predicted noise)\n",
        "            predicted_noise = self.model(img, timesteps, embedding)\n",
        "            # Compute denoise step\n",
        "            alpha = self.alphas[i]\n",
        "            alpha_cumprod = self.alphas_cumprod[i]\n",
        "            beta = self.betas[i]\n",
        "            if i > 0:\n",
        "                noise = torch.randn_like(img)\n",
        "            else:\n",
        "                noise = torch.zeros_like(img)\n",
        "            # Compute step according to the formula\n",
        "            img = (1 / torch.sqrt(alpha)) * (\n",
        "                img - ((1 - alpha) / torch.sqrt(1 - alpha_cumprod)) * predicted_noise\n",
        "            ) + torch.sqrt(beta) * noise\n",
        "        # Scale image to [0, 1]\n",
        "        img = (img.clamp(-1, 1) + 1) / 2\n",
        "        return img\n",
        "\n",
        "def train_face_generation(\n",
        "    data_dir='/content/drive/MyDrive/img_align_celeba',\n",
        "    output_dir='outputs',\n",
        "    batch_size=32,\n",
        "    img_size=224,\n",
        "    embedding_dim=768,\n",
        "    encoder_epochs=10,\n",
        "    diffusion_epochs=10,\n",
        "    device='cuda'\n",
        "):\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    # Get dataloaders\n",
        "    train_loader, val_loader = get_dataloaders(data_dir, batch_size=batch_size, img_size=img_size)\n",
        "    # Initialize encoder\n",
        "    encoder = DINOv2FaceEncoder(embedding_dim=embedding_dim, finetune=True).to(device)\n",
        "    # Initialize diffusion model\n",
        "    unet = ConditionalUNet(\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        base_channels=64,\n",
        "        time_dim=256,\n",
        "        embedding_dim=embedding_dim,\n",
        "        device=device\n",
        "    ).to(device)\n",
        "    diffusion = DiffusionModel(unet, num_diffusion_steps=1000, device=device)\n",
        "    # Optimizers and loss\n",
        "    encoder_optimizer = optim.AdamW(encoder.parameters(), lr=1e-4)\n",
        "    diffusion_optimizer = optim.AdamW(diffusion.model.parameters(), lr=2e-4)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # For tracking loss per epoch\n",
        "    loss_history = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(diffusion_epochs):\n",
        "        encoder.train()\n",
        "        diffusion.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{diffusion_epochs}\"):\n",
        "            images = batch.to(device)\n",
        "            # Generate embeddings (without gradient computation for efficiency)\n",
        "            with torch.no_grad():\n",
        "                embeddings = encoder(images)\n",
        "            # Train diffusion model\n",
        "            loss = diffusion.train_step(images, embeddings, diffusion_optimizer, criterion)\n",
        "            total_loss += loss\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        loss_history.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Periodically generate and save samples\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            diffusion.model.eval()\n",
        "            with torch.no_grad():\n",
        "                # Sample some embeddings from validation set\n",
        "                val_batch = next(iter(val_loader)).to(device)\n",
        "                val_embeddings = encoder(val_batch)\n",
        "                # Generate samples\n",
        "                samples = diffusion.sample(val_embeddings[:16], image_size=img_size)\n",
        "                # Save samples\n",
        "                save_path = os.path.join(output_dir, f'samples_epoch_{epoch+1}.png')\n",
        "                plt.figure(figsize=(10, 10))\n",
        "                grid = make_grid(samples, nrow=4, normalize=True)\n",
        "                plt.imshow(grid.permute(1, 2, 0).cpu())\n",
        "                plt.axis('off')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(save_path)\n",
        "                plt.close()\n",
        "\n",
        "    # Save final models\n",
        "    torch.save({\n",
        "        'encoder': encoder.state_dict(),\n",
        "        'diffusion': diffusion.model.state_dict()\n",
        "    }, os.path.join(output_dir, 'final_model.pth'))\n",
        "\n",
        "    # Plot and save the training loss curve\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(1, diffusion_epochs+1), loss_history, marker='o')\n",
        "    plt.title(\"Training Loss per Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Average Loss\")\n",
        "    plt.grid(True)\n",
        "    loss_plot_path = os.path.join(output_dir, \"training_loss.png\")\n",
        "    plt.savefig(loss_plot_path)\n",
        "    plt.close()\n",
        "\n",
        "    return encoder, diffusion\n",
        "\n",
        "def make_grid(tensor, nrow=8, padding=2, normalize=False, scale_each=False, pad_value=0):\n",
        "    \"\"\"\n",
        "    Make a grid of images from a tensor\n",
        "    \"\"\"\n",
        "    from torchvision.utils import make_grid as tv_make_grid\n",
        "    return tv_make_grid(tensor, nrow=nrow, padding=padding,\n",
        "                        normalize=normalize,\n",
        "                        scale_each=scale_each,\n",
        "                        pad_value=pad_value)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Specific path to face dataset\n",
        "    data_dir = '/content/drive/MyDrive/img_align_celeba'\n",
        "    # Train the face generation model\n",
        "    encoder, diffusion_model = train_face_generation(\n",
        "        data_dir=data_dir,\n",
        "        output_dir='face_generation_outputs',\n",
        "        batch_size=32,\n",
        "        img_size=224,\n",
        "        embedding_dim=768,\n",
        "        encoder_epochs=10,\n",
        "        diffusion_epochs=10\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgK8hQfiH-hr",
        "outputId": "725466ea-9765-456f-83b4-5ba26abfdd45"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5434 images in /content/drive/MyDrive/img_align_celeba\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 153/153 [02:39<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 0.3279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 153/153 [01:47<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Average Loss: 0.1126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 153/153 [01:47<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Average Loss: 0.0873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 153/153 [01:47<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Average Loss: 0.0737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 153/153 [01:47<00:00,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Average Loss: 0.0659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Sampling: 1000it [00:04, 220.37it/s]\n",
            "Epoch 6/10: 100%|██████████| 153/153 [01:47<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Average Loss: 0.0576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 153/153 [02:37<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Average Loss: 0.0548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 153/153 [01:47<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Average Loss: 0.0508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 153/153 [01:47<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Average Loss: 0.0462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 153/153 [01:47<00:00,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Average Loss: 0.0500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Sampling: 1000it [00:04, 219.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uDkFyvTiH_LR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}